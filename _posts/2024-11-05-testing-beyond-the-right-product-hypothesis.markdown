Let's think about testing beyond the "Is this the right product?" hypotheses.
=========================
![A weasel contemplating a web page](/assets/weaseltestingawebpage.jpeg)
_The image above was generated by Bing Image Creator, which I prompted for a 
photo of a weasel looking at a webpage with a banner that was messing it up somehow. This is in homage to Alan
Page's article mentioned below, and his self-reference as "The Angry Weasel."_

Recently, Alan Page wrote a <a href="https://angryweasel.substack.com/p/the-other-side-of-the-door?r=ngs26&utm_campaign=post&utm_medium=web">useful and well put article</a>
about testing product value via hypotheses. It is a straightforward, easy to imagine and understand
how-to method. Good stuff. I recommend giving it a read.

I want to follow it up with considering testing the example offered in the article
in ways that go beyond determining the product value. I want to talk about
the what a person testing the product, be that a developer or a tester
or any other member of the team, does in a way that complements
and assists the goal.

The Banner Example
=========================
In Alan's article, we are presented with a proposed addition of a banner
to a website. The purpose of the banner is to drive up user email
list sign-ups. The article then describes a hypothesis (and experiment)
driven approach to determine whether or not the change produces the
desired value, in this case driving up email-list sign-ups.

Let's frame that as a set of goals, both explicit and implicit:
- an effective experiment which can give us data we can use to test the hypothesis
- ultimately an increase in email-list sign-ups, with the experiment providing us the data to see if that relates to the banner change
- no negative value impact of the change or experiment on the rest of the site

To our business, these goals express a value of this experiment, this change, to
the business and to the customer. When we test, we are looking for something that
might threaten that value, might keep us from meeting those goals.

What Do We Test?
=========================
What to test is not as easy a problem as it might sound. Given the goals,
we might be fine with a few quick checks from
whomever is creating the banner before letting it go into production. Or
we might need to take a deep look at the banner and its interaction
with the rest of the site, customer environments, and usage patterns. How
do we decide?

It all depends.

Consider the situation
-------------------------
Our example is vague, and there are a lot of conditions which
would be more apparent were this a real project we were really
working on. Consider:
- Is this a static page or is the page content created by a more complex, dynamic process?
- Is putting a banner on our pages a new thing, or is it something we don all the time with tools we already know and understand?
- How complex is the site? Content only, or a richly functional page with lots of complex behavior?
- What is at risk of something doesn't work? Is there money on the line, customer data, loss of time and effort for ourselves and the customer?
- What is our configuration and environment distribution? Many platforms and browsers on many versions, or more singular and narrow?
- What is the world-wide scope of the site, in how many countries and languages?
- How much existing coverage do we have already to check changes for regression? What is our current rate of failure and in production defect, ability to respond?

We can consider a lot of other questions, all of which will have a big
impact on the relative risk of the change. Maybe adding this banner is
a great big nothing burger, or maybe it means a lot of work building
something new that is going to be difficult to integrate with the existing
site. It is cliche to say "you have to ask," but in truth, if you
work on the team you probably already know, or if you don't
you should find out soon.

Consider the potential risks
-------------------------
